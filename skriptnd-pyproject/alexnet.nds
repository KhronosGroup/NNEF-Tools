import nn;
import layout;

graph AlexNet {
    @attrib {
        batch: int = 1;
        classes: int = 1000;
    }
    @input {
        input: real[batch,3,224,224];
    }
    @output {
        output: real[batch,classes];
    }
    @variable {
        kernel1: real[64, 3, 11, 11];
        bias1: real[64];
        kernel2: real[192, 64, 5, 5];
        bias2: real[192];
        kernel3: real[384, 192, 3, 3];
        bias3: real[384];
        kernel4: real[384, 384, 3, 3];
        bias4: real[384];
        kernel5: real[256, 384, 3, 3];
        bias5: real[256];
        kernel6: real[4096, 256, 5, 5];
        bias6: real[4096];
        kernel7: real[4096, 4096];
        bias7: real[4096];
        kernel8: real[classes, 4096];
        bias8: real[classes];
    }
    @compose {
        conv1 = nn.conv{padding=0, stride=4}(input, kernel1, bias1);
        relu1 = nn.relu(conv1);
        pool1 = nn.max_pool{padding=0, size=3, stride=2}(relu1);
        conv2 = nn.conv{padding=2}(pool1, kernel2, bias2);
        relu2 = nn.relu(conv2);
        pool2 = nn.max_pool{padding=0, size=3, stride=2}(relu2);
        conv3 = nn.conv{padding=1}(pool2, kernel3, bias3);
        relu3 = nn.relu(conv3);
        conv4 = nn.conv{padding=1}(relu3, kernel4, bias4);
        relu4 = nn.relu(conv4);
        conv5 = nn.conv{padding=1}(relu4, kernel5, bias5);
        relu5 = nn.relu(conv5);
        pool3 = nn.max_pool{padding=0, size=3, stride=2}(relu5);
        conv6 = nn.conv{padding=0}(pool3, kernel6, bias6);
        relu6 = nn.relu(conv6);
        flat1 = layout.flatten{axis=1}(relu6);
        conv7 = nn.linear(flat1, kernel7, bias7);
        relu7 = nn.relu(conv7);
        conv8 = nn.linear(relu7, kernel8, bias8);
        output = nn.softmax(conv8);
    }
}
